# AHF - AI Hub Framework

> A comprehensive, modular, and extensible Python framework for building AI-powered applications with LLMs, Agents, Tools, and Prompt Management.

## ğŸ—ï¸ Architecture Overview

```
AHF/
â”œâ”€â”€ core/                    # Core framework modules
â”‚   â”œâ”€â”€ llms/               # LLM providers and interfaces
â”‚   â”œâ”€â”€ agents/             # Agent implementations
â”‚   â”œâ”€â”€ tools/              # Tool execution system
â”‚   â””â”€â”€ promptregistry/     # Prompt management system
â”œâ”€â”€ utils/                   # Shared utilities
â”‚   â”œâ”€â”€ logging/            # Logging infrastructure
â”‚   â”œâ”€â”€ converters/         # Data converters
â”‚   â””â”€â”€ circuitBreaker/     # Circuit breaker pattern
â””â”€â”€ tests/                   # Test suites
```

---

## ğŸ“¦ Core Modules

### 1. LLMs Module (`core/llms/`)

**Purpose**: Unified interface for Large Language Models from different providers.

#### Key Interfaces

```python
from core.llms import ILLM, LLMContext, LLMResponse

# ILLM Protocol - All LLMs implement just 2 methods:
class ILLM(Protocol):
    async def get_answer(self, messages, ctx, **kwargs) -> LLMResponse: ...
    async def stream_answer(self, messages, ctx, **kwargs) -> AsyncIterator[LLMStreamChunk]: ...
```

#### Pluggable Components

| Component | Interface | Implementations |
|-----------|-----------|-----------------|
| Validator | `ILLMValidator` | `BasicLLMValidator`, `NoOpLLMValidator` |
| Transformer | `IParameterTransformer` | `AzureGPT4Transformer`, `NoOpTransformer` |
| Parser | `IResponseParser` | `AzureResponseParser`, `NoOpResponseParser` |
| Structured Handler | `IStructuredOutputHandler` | `BasicStructuredHandler`, `NoOpStructuredHandler` |

#### Usage Example

```python
from core.llms import LLMFactory, LLMContext, LLMValidatorFactory

# Create LLM
llm = LLMFactory.create_llm(
    "gpt-4o",
    connector_config={"api_key": "sk-..."}
)

# With custom validator
llm = LLMFactory.create_llm(
    "gpt-4o",
    connector_config={"api_key": "sk-..."},
    validator=LLMValidatorFactory.get_validator('noop')
)

# Get answer
messages = [{"role": "user", "content": "Hello!"}]
response = await llm.get_answer(messages, LLMContext(), temperature=0.7)
print(response.content)

# Stream answer
async for chunk in llm.stream_answer(messages, LLMContext()):
    print(chunk.content, end='', flush=True)
```

#### Key Models

- `LLMResponse`: Complete response with content, usage, metadata
- `LLMStreamChunk`: Streaming chunk with partial content
- `LLMUsage`: Token counts, latency, cost tracking
- `LLMContext`: Request context with tracing, user info
- `ModelMetadata`: Model capabilities and constraints

#### Providers

- **Azure OpenAI**: `AzureConnector`, `AzureLLM` (GPT-4, GPT-4o, etc.)
- **OpenAI**: Placeholder (not yet migrated)

---

### 2. Agents Module (`core/agents/`)

**Purpose**: Flexible, pluggable architecture for intelligent agents.

#### Agent Types

| Type | Description | Use Case |
|------|-------------|----------|
| `SimpleAgent` | Single-shot Q&A | Quick queries |
| `ReactAgent` | Reason-Act loop | Multi-step reasoning with tools |
| `GoalBasedAgent` | Goal-driven with checklist | Complex tasks with progress tracking |
| `HierarchicalAgent` | Manager delegates to workers | Large-scale task decomposition |

#### Key Interfaces

```python
from core.agents import IAgent, AgentContext, AgentResult

# IAgent Protocol
class IAgent(Protocol):
    async def run(self, input_data, ctx, **kwargs) -> AgentResult: ...
    async def stream(self, input_data, ctx, **kwargs) -> AsyncIterator[AgentStreamChunk]: ...
    def get_state(self) -> Dict[str, Any]: ...
```

#### Pluggable Components

| Component | Interface | Implementations |
|-----------|-----------|-----------------|
| Memory | `IAgentMemory` | `DictMemory`, `NoOpAgentMemory` |
| Scratchpad | `IAgentScratchpad` | `BasicScratchpad`, `StructuredScratchpad` |
| Checklist | `IAgentChecklist` | `BasicChecklist` |
| Observer | `IAgentObserver` | `LoggingObserver`, `NoOpObserver` |
| Input Processor | `IAgentInputProcessor` | Custom implementations |
| Output Processor | `IAgentOutputProcessor` | Custom implementations |

#### Builder Pattern

```python
from core.agents import AgentBuilder, AgentType, DictMemory, BasicScratchpad

agent = (AgentBuilder()
    .with_name("code_assistant")
    .with_llm(llm)
    .with_tools([search_tool, code_runner])
    .with_memory(DictMemory())
    .with_scratchpad(BasicScratchpad())
    .with_max_iterations(10)
    .with_input_types([AgentInputType.TEXT])
    .with_output_types([AgentOutputType.TEXT, AgentOutputType.STRUCTURED])
    .as_type(AgentType.REACT)
    .build())

result = await agent.run("Write a Python function to sort a list", ctx)
```

#### Factory Pattern

```python
from core.agents import AgentFactory

# Create agent by type
agent = AgentFactory.create(
    agent_type=AgentType.REACT,
    llm=llm,
    tools=tools,
    max_iterations=10
)
```

---

### 3. Tools Module (`core/tools/`)

**Purpose**: Modular, async tool execution with validation, security, and policies.

#### Tool Types

| Type | Class | Description |
|------|-------|-------------|
| Function | `FunctionToolSpec` | Python function calls |
| HTTP | `HttpToolSpec` | REST API calls |
| Database | `DbToolSpec` | Database operations |

#### Key Interfaces

```python
from core.tools import IToolExecutor, ToolContext, ToolResult

# IToolExecutor Protocol
class IToolExecutor(Protocol):
    async def execute(self, args: Dict, ctx: ToolContext) -> ToolResult: ...
```

#### Pluggable Components

| Component | Interface | Implementations |
|-----------|-----------|-----------------|
| Validator | `IToolValidator` | `BasicValidator`, `NoOpValidator` |
| Security | `IToolSecurity` | `BasicSecurity`, `NoOpSecurity` |
| Policy | `IToolPolicy` | `NoOpPolicy`, Circuit Breakers, Retry Policies |
| Emitter | `IToolEmitter` | `NoOpEmitter` |
| Memory | `IToolMemory` | `NoOpMemory` |
| Metrics | `IToolMetrics` | `NoOpMetrics` |
| Tracer | `IToolTracer` | `NoOpTracer` |
| Limiter | `IToolLimiter` | `NoOpLimiter` |

#### Policies

**Retry Policies**:
- `NoRetryPolicy`: No retries
- `FixedRetryPolicy`: Fixed delay between retries
- `ExponentialBackoffRetryPolicy`: Exponential backoff
- `CustomRetryPolicy`: User-defined retry logic

**Circuit Breaker**:
- `CircuitBreakerPolicy`: Fail-fast when service is unhealthy
- States: CLOSED â†’ OPEN â†’ HALF_OPEN

#### Idempotency Strategies

| Strategy | Description |
|----------|-------------|
| `DefaultIdempotencyKeyGen` | Hash of all parameters |
| `FieldIdempotencyKeyGen` | Hash of specific fields |
| `HashIdempotencyKeyGen` | Custom hash function |
| `CustomIdempotencyKeyGen` | User-defined generator |

#### Usage Example

```python
from core.tools import (
    ToolSpec, ToolParameter, ToolType, ToolContext,
    FunctionToolExecutor, BasicValidator
)

# Define tool spec
spec = ToolSpec(
    tool_name="calculator",
    description="Performs arithmetic operations",
    tool_type=ToolType.FUNCTION,
    parameters=[
        ToolParameter(name="operation", type="string", required=True),
        ToolParameter(name="a", type="number", required=True),
        ToolParameter(name="b", type="number", required=True),
    ]
)

# Create executor
executor = FunctionToolExecutor(
    spec=spec,
    func=calculate,
    validator=BasicValidator()
)

# Execute
result = await executor.execute(
    {"operation": "add", "a": 5, "b": 3},
    ToolContext(user_id="user-123")
)
print(result.output)  # 8
```

#### Serialization

```python
from core.tools import tool_to_json, tool_from_json, tool_to_dict

# Serialize for LLM function calling
json_schema = tool_to_json(tool_spec)
dict_schema = tool_to_dict(tool_spec)

# Deserialize
spec = tool_from_json(json_schema)
```

---

### 4. Prompt Registry Module (`core/promptregistry/`)

**Purpose**: Centralized prompt management with versioning, environments, and metrics.

#### Features

- âœ… **Versioning**: Immutable prompt versions
- âœ… **LLM-Specific**: Different prompts per model
- âœ… **Environments**: prod, staging, dev, test with fallback
- âœ… **Dynamic Variables**: Template substitution `{variable}`
- âœ… **Evaluation Scores**: LLM and human eval tracking
- âœ… **Runtime Metrics**: Latency, tokens, cost, success rate
- âœ… **Validation**: Input sanitization, prompt injection protection
- âœ… **Security**: Role-based access control
- âœ… **Storage**: Pluggable backends (JSON/YAML local, future: S3)

#### Key Interfaces

```python
from core.promptregistry import IPromptRegistry, IPromptStorage, IPromptValidator, IPromptSecurity

# IPromptRegistry Protocol
class IPromptRegistry(Protocol):
    async def save_prompt(self, label, content, metadata, security_context) -> str: ...
    async def get_prompt(self, label, version, model, environment, variables, security_context) -> str: ...
    async def get_prompt_with_fallback(self, label, ...) -> PromptRetrievalResult: ...
    async def record_usage(self, prompt_id, latency_ms, prompt_tokens, completion_tokens, cost, success) -> None: ...
    async def update_eval_scores(self, prompt_id, llm_eval_score, human_eval_score) -> None: ...
```

#### Pluggable Components

| Component | Interface | Implementations |
|-----------|-----------|-----------------|
| Storage | `IPromptStorage` | `LocalFileStorage` (JSON/YAML) |
| Validator | `IPromptValidator` | `NoOpPromptValidator`, `BasicPromptValidator` |
| Security | `IPromptSecurity` | `NoOpPromptSecurity`, `RoleBasedPromptSecurity` |

#### Core Models

```python
from core.promptregistry import (
    PromptMetadata,      # Version, model, environment, eval scores
    PromptEntry,         # Label with multiple versions
    PromptVersion,       # Single immutable version
    PromptTemplate,      # Content with dynamic variables
    RuntimeMetrics,      # Usage tracking (latency, tokens, cost)
    PromptRetrievalResult  # Retrieved prompt with metadata
)
```

#### Enums

```python
from core.promptregistry import PromptEnvironment, PromptType, PromptStatus, PromptCategory

PromptEnvironment.PROD      # Production
PromptEnvironment.STAGING   # Staging
PromptEnvironment.DEV       # Development
PromptEnvironment.TEST      # Testing

PromptType.SYSTEM          # System prompt
PromptType.USER            # User prompt
```

#### Usage Examples

**Basic Usage**:
```python
from core.promptregistry import LocalPromptRegistry, PromptMetadata, PromptEnvironment, PromptType

registry = LocalPromptRegistry(storage_path=".prompts")

# Save prompt
prompt_id = await registry.save_prompt(
    label="code_review",
    content="Review this {language} code for {review_type}: {code}",
    metadata=PromptMetadata(
        model_target="gpt-4",
        environment=PromptEnvironment.PROD,
        prompt_type=PromptType.SYSTEM,
        tags=["code", "review"],
        llm_eval_score=0.95
    )
)

# Get prompt with variables
content = await registry.get_prompt(
    "code_review",
    variables={"language": "Python", "review_type": "bugs", "code": "..."}
)

# Get with fallback and full metadata
result = await registry.get_prompt_with_fallback(
    "code_review",
    model="gpt-4",
    environment=PromptEnvironment.PROD,
    variables={"language": "Python", "review_type": "bugs", "code": "..."}
)
print(result.content)
print(result.version)
print(result.fallback_used)
```

**With Validation**:
```python
from core.promptregistry import BasicPromptValidator

validator = BasicPromptValidator(
    max_content_length=50000,
    max_variable_length=1000,
    blocked_patterns=[r"ignore.*previous.*instructions"]  # Prevent injection
)

registry = LocalPromptRegistry(
    storage_path=".prompts",
    validator=validator
)
```

**With Security**:
```python
from core.promptregistry import RoleBasedPromptSecurity, SecurityContext

security = RoleBasedPromptSecurity(
    read_roles=["*"],           # Everyone can read
    write_roles=["admin", "developer"],
    delete_roles=["admin"]
)

registry = LocalPromptRegistry(
    storage_path=".prompts",
    security=security
)

# Use with security context
ctx = SecurityContext(user_id="user123", roles=["developer"])
await registry.save_prompt("test", "content", security_context=ctx)
```

**LLM Integration**:
```python
from core.promptregistry import PromptAwareLLM, call_with_prompt

# Option 1: Wrap LLM for automatic tracking
tracked_llm = PromptAwareLLM(llm, registry)
response = await tracked_llm.get_answer(messages, ctx, prompt_id=prompt_id)

# Option 2: Convenience function
response, prompt_id = await call_with_prompt(
    llm=llm,
    registry=registry,
    prompt_label="code_review",
    user_message="Review this code...",
    ctx=ctx,
    variables={"language": "Python"}
)
```

**Runtime Metrics**:
```python
# Record usage after LLM call
await registry.record_usage(
    prompt_id,
    latency_ms=150.0,
    prompt_tokens=100,
    completion_tokens=50,
    cost=0.002,
    success=True
)

# Get metrics
metrics = await registry.get_runtime_metrics(prompt_id)
print(f"Usage count: {metrics.usage_count}")
print(f"Avg latency: {metrics.avg_latency_ms}ms")
print(f"Success rate: {metrics.success_rate:.1%}")

# Update eval scores
await registry.update_eval_scores(
    prompt_id,
    llm_eval_score=0.92,
    human_eval_score=0.88
)
```

---

## ğŸ› ï¸ Utilities (`utils/`)

### Logging System

```python
from utils.logging import LoggerAdaptor, DurationLogger, DelayedLogger, ConfigManager

# Environment-aware configuration
config = ConfigManager.get_config("prod")  # Loads log_config_prod.json
logger = LoggerAdaptor.get_logger("my_service", config)

# Duration tracking
with DurationLogger("operation_name", logger):
    # ... operation ...
    pass  # Logs duration automatically

# Delayed logging (batch logs)
delayed = DelayedLogger(logger, batch_size=100)
delayed.info("Message")
delayed.flush()
```

### Circuit Breaker

```python
from utils.circuitBreaker import CircuitBreaker

breaker = CircuitBreaker(
    failure_threshold=5,
    recovery_timeout=30,
    half_open_max_calls=3
)

@breaker
async def call_external_service():
    # ... service call ...
    pass
```

### JSON Converters

```python
from utils.converters import partial_json_parser, json_schema_converter

# Parse partial/streaming JSON
partial = '{"name": "test", "value":'
result = partial_json_parser.parse(partial)

# Convert between JSON schemas
converted = json_schema_converter.convert(schema, target_format)
```

---

## ğŸ“ Directory Structure Details

```
AHF/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ llms/
â”‚   â”‚   â”œâ”€â”€ __init__.py              # Public exports
â”‚   â”‚   â”œâ”€â”€ interfaces/              # ILLM, IConnector, etc.
â”‚   â”‚   â”œâ”€â”€ spec/                    # LLMResponse, LLMContext, etc.
â”‚   â”‚   â”œâ”€â”€ enum.py                  # LLMProvider, FinishReason, etc.
â”‚   â”‚   â”œâ”€â”€ exceptions.py            # LLMError, RateLimitError, etc.
â”‚   â”‚   â”œâ”€â”€ constants.py             # Default values
â”‚   â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â”‚   â”œâ”€â”€ base/                # BaseLLM, BaseConnector
â”‚   â”‚   â”‚   â””â”€â”€ azure/               # AzureConnector, AzureLLM
â”‚   â”‚   â””â”€â”€ runtimes/                # Validators, Transformers, Parsers
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py              # Public exports
â”‚   â”‚   â”œâ”€â”€ interfaces/              # IAgent, IAgentMemory, etc.
â”‚   â”‚   â”œâ”€â”€ spec/                    # AgentSpec, AgentResult, etc.
â”‚   â”‚   â”œâ”€â”€ enum.py                  # AgentType, AgentState
â”‚   â”‚   â”œâ”€â”€ exceptions.py            # AgentError, MaxIterationsError
â”‚   â”‚   â”œâ”€â”€ constants.py             # Default values
â”‚   â”‚   â”œâ”€â”€ builders/                # AgentBuilder, AgentContextBuilder
â”‚   â”‚   â”œâ”€â”€ implementations/         # SimpleAgent, ReactAgent, etc.
â”‚   â”‚   â””â”€â”€ runtimes/                # Memory, Scratchpad, Checklist
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ __init__.py              # Public exports
â”‚   â”‚   â”œâ”€â”€ interfaces/              # IToolExecutor, IToolValidator
â”‚   â”‚   â”œâ”€â”€ spec/                    # ToolSpec, ToolResult, etc.
â”‚   â”‚   â”œâ”€â”€ enum.py                  # ToolType, ToolReturnType
â”‚   â”‚   â”œâ”€â”€ serializers/             # tool_to_json, tool_from_json
â”‚   â”‚   â””â”€â”€ runtimes/
â”‚   â”‚       â”œâ”€â”€ executors/           # Function, HTTP, DB executors
â”‚   â”‚       â”œâ”€â”€ validators/          # BasicValidator, NoOpValidator
â”‚   â”‚       â”œâ”€â”€ security/            # BasicSecurity, NoOpSecurity
â”‚   â”‚       â”œâ”€â”€ policies/            # Retry, Circuit Breaker
â”‚   â”‚       â””â”€â”€ idempotency/         # Key generators
â”‚   â”‚
â”‚   â””â”€â”€ promptregistry/
â”‚       â”œâ”€â”€ __init__.py              # Public exports
â”‚       â”œâ”€â”€ interfaces/              # IPromptRegistry, IPromptStorage
â”‚       â”œâ”€â”€ spec/                    # PromptMetadata, PromptEntry
â”‚       â”œâ”€â”€ enum.py                  # PromptEnvironment, PromptType
â”‚       â”œâ”€â”€ constants.py             # Default values
â”‚       â”œâ”€â”€ defaults/                # Default agent prompts
â”‚       â””â”€â”€ runtimes/
â”‚           â”œâ”€â”€ storage/             # LocalFileStorage, LocalPromptRegistry
â”‚           â”œâ”€â”€ validators/          # NoOp, Basic validators
â”‚           â”œâ”€â”€ security/            # NoOp, RoleBased security
â”‚           â””â”€â”€ llm_integration.py   # PromptAwareLLM helper
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logging/                     # LoggerAdaptor, DurationLogger
â”‚   â”œâ”€â”€ converters/                  # JSON parsers and converters
â”‚   â””â”€â”€ circuitBreaker/              # Circuit breaker pattern
â”‚
â””â”€â”€ tests/                           # Test suites for all modules
```

---

## ğŸ”§ Design Principles

### 1. Interface-Driven (SOLID)

All components define interfaces (Protocols) before implementations:
- Easy to swap implementations
- Clear contracts between components
- Facilitates testing with mocks

### 2. Pluggable Components

Every module supports pluggable components via factories:
```python
# Validators
validator = ValidatorFactory.get_validator('basic')  # or 'noop'

# Security
security = SecurityFactory.get_security('role_based')  # or 'noop'

# Storage
storage = StorageFactory.get_storage('local')  # or future: 's3'
```

### 3. NoOp Implementations

Every interface has a NoOp implementation for:
- Development without dependencies
- Testing without side effects
- Gradual feature enablement

### 4. Factory Pattern

Consistent factory pattern across all modules:
```python
LLMFactory.create_llm(...)
AgentFactory.create(...)
ExecutorFactory.create(...)
PromptRegistryFactory.get_registry(...)
```

### 5. Builder Pattern

Complex objects use builders for fluent construction:
```python
agent = (AgentBuilder()
    .with_name("agent")
    .with_llm(llm)
    .with_tools(tools)
    .build())
```

---

## ğŸ§ª Testing

### Run All Tests
```bash
uv run pytest tests/ -v
```

### Run Module Tests
```bash
uv run pytest tests/llms/ -v
uv run pytest tests/agents/ -v
uv run pytest tests/tools/ -v
uv run pytest tests/promptregistry/ -v
```

### Run Specific Test
```bash
uv run pytest tests/promptregistry/test_prompt_registry.py::TestLocalPromptRegistry -v
```

### Test Markers
```bash
uv run pytest -m "unit"        # Unit tests only
uv run pytest -m "integration" # Integration tests
uv run pytest -m "slow"        # Slow tests
```

---

## ğŸ“š Quick Reference

### Imports Cheat Sheet

```python
# LLMs
from core.llms import (
    ILLM, LLMContext, LLMResponse, LLMUsage,
    LLMFactory, BaseLLM, AzureLLM
)

# Agents
from core.agents import (
    IAgent, AgentContext, AgentResult,
    AgentBuilder, AgentType, AgentFactory,
    SimpleAgent, ReactAgent, GoalBasedAgent,
    DictMemory, BasicScratchpad
)

# Tools
from core.tools import (
    IToolExecutor, ToolSpec, ToolContext, ToolResult,
    ExecutorFactory, FunctionToolExecutor,
    BasicValidator, NoOpValidator
)

# Prompt Registry
from core.promptregistry import (
    IPromptRegistry, LocalPromptRegistry,
    PromptMetadata, PromptEnvironment, PromptType,
    BasicPromptValidator, RoleBasedPromptSecurity,
    PromptAwareLLM, call_with_prompt
)
```

---

## ğŸš€ Getting Started

1. **Install dependencies**:
   ```bash
   uv sync
   ```

2. **Create an LLM**:
   ```python
   from core.llms import LLMFactory, LLMContext
   
   llm = LLMFactory.create_llm("gpt-4o", connector_config={...})
   ```

3. **Create an Agent**:
   ```python
   from core.agents import AgentBuilder, AgentType
   
   agent = AgentBuilder().with_llm(llm).as_type(AgentType.REACT).build()
   ```

4. **Use Prompt Registry**:
   ```python
   from core.promptregistry import LocalPromptRegistry
   
   registry = LocalPromptRegistry(storage_path=".prompts")
   await registry.save_prompt("greeting", "Hello {name}!")
   ```

5. **Run**:
   ```python
   result = await agent.run("Your question here", AgentContext())
   print(result.content)
   ```
